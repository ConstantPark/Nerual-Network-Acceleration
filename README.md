## Neural Network Acceleration Study
This is a repository of the study "neural network acceleration". The goal of this study is to understand the acceleration of nerual networks on various devices. The topic of acceleration includes `CPU`,`GPU`, `FPGA`, `ASIC` , `NPU` and `PIM`. Our materials are open to this github and youtube.

#### CPU/GPU and NPU
- Desinging optimized BLAS for CPU or GPU
- Optimal primitive selection on heterogeneous system architecture (HSA) device
- CUDA/OpenCL kernel design

#### ASIC and FPGA
- Low-power inference acceleration using HLS or RTL design
- High computing performance training accelerator

#### PIM (NDP)
- DIMM and HMC based neural acceleration system
- Non-HBM based design

## Paper List (20)
### CPU and GPU (8)
	1. AccUDNN: A GPU Memory Efficient Accelerator for Training Ultra-deep Neural Networks, arxiv, 2019.
	2. ÂµLayer:Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization, EuroSys, 2019.
	3. Partitioning Compute Units in CNN Acceleration for Statistical Memory Traffic Shaping, IEEE CAL, 2017.
	4. MOSAIC: Heterogeneity-, Communication-, and Constraint-Aware Model Slicing and Execution for Accurate and Efficient Inference, PACT, 2019.
	5. Optimal DNN Primitive Selection with Partitioned Boolean quadratic Programming, ACM CGO, 2019.
	6. Neural Network Inference on Mobile SoCs, Arxiv 2019.
	7. Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems, DATE, 2019.
	8. Performance analysis of CNN frameworks for GPUs, ISPASS, 2018.
  

### ASIC and FPGA (9)
	1. Cambricon: An instruction set architecture for neural networks, ISCA, 2016.
	2. Caffeine: Towards Uniformed Representation and Acceleration for Deep Convolutional Neural Networks, ICCAD, 2016.
	3. EIE: efficient inference engine on compressed deep neural network, ISCA, 2016.
	4. ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA, FPGA, 2017.
	5. In-Datacenter Performance Analysis of a Tensor Processing Unit, ISCA, 2017.
	6. Overcoming Data Transfer Bottlenecks in FPGA-based DNN Accelerators via Layer Conscious Memory Management, DAC, 2019.
	7. Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks, FPGA, 2015.
	8. FA3C: FPGA-Accelerated Deep Reinforcement Learning, ASPLOS, 2019.
	9. Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach, MICRO, 2018.
### PIM (3)
	1. Tetris: Scalable and Efficient Neural Network Acceleration with 3D Memory, ASPLOS, 2017.
	2. Processing-in-Memory for Energy-efficient Neural Network Training: A Heterogeneous Approach, MICRO, 2018.
	3. TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning, MICRO, 2019.


## Presentation with Video
### Week1: Introduction of Neural network acceleration
**Title: Going to fast acceleration on your devicet**  
Presentor: Constant Park (Sang-Soo Park)  
PPT:   
Video:   
Date:  

## Contributors
**Main Contributor**: Constant Park (https://constantpark.github.io/).  
**Presenter**: Constanr Park, ~

